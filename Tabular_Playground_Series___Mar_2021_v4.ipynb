{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tabular_Playground_Series_-_Mar_2021_v4",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "15SYMGx21qM85lA1s6uNeOXHsFPaSRgPJ",
      "authorship_tag": "ABX9TyP7KyGiYufmpBk1AtLF78Aa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomohiroYazaki/Tabular_Playground_Series_-_Mar_2021/blob/main/Tabular_Playground_Series___Mar_2021_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNn-U7LiGpBx"
      },
      "source": [
        "!pip install optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP8Htkhtzi6b"
      },
      "source": [
        "#!pip install --upgrade pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6hKguhK8U6K"
      },
      "source": [
        "!pip install pytorch-tabnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UI8gqgvUusX"
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2xEMsMpjRBz"
      },
      "source": [
        "!pip install category_encoders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSMekwR-eL_a"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import statistics\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import time\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "#import os\n",
        "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#    for filename in filenames:\n",
        "#        print(os.path.join(dirname, filename))\n",
        "        \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import metrics\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "\n",
        "import optuna\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "        \n",
        "#input_path = Path('/kaggle/input/tabular-playground-series-jan-2021/')\n",
        "input_path = Path('/content/drive/MyDrive/Kaggle/Tabular_Playground_Series_-_Mar_2021/Data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ko1zSMe8bDj"
      },
      "source": [
        "**---------- Utilities ----------**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6QPy6pf9G0q"
      },
      "source": [
        "!rm -f /content/log.log\n",
        "!rm -f /content/result.png\n",
        "!rm -f /content/submission.csv\n",
        "!rm -f /content/submission_mean.csv\n",
        "!rm -f /content/submission_stack.csv\n",
        "!rm -f /content/plot_optimization_history.html\n",
        "!rm -f /content/plot_parallel_coordinate.html\n",
        "!rm -f /content/plot_slice.html\n",
        "!rm -f /content/plot_param_importances.html\n",
        "!rm -f /content/plot_contour.html\n",
        "!rm -f /content/best_params.json\n",
        "!rm -f /content/FEATURES_SEARCH.csv\n",
        "!rm -f /content/FEATURES_SEARCH.png\n",
        "!rm -f /content/train.csv\n",
        "!rm -f /content/test.csv\n",
        "!rm -f /content/target.csv\n",
        "!rm -f -r /content/TEST\n",
        "!rm -f -r /content/OOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo_Uu2jE9LnL"
      },
      "source": [
        "def get_logger(filename='log'):\n",
        "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
        "    logger = getLogger(__name__)\n",
        "    logger.setLevel(INFO)\n",
        "    #handler1 = StreamHandler()\n",
        "    #handler1.setFormatter(Formatter(\"%(message)s\"))\n",
        "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
        "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
        "    #logger.addHandler(handler1)\n",
        "    logger.addHandler(handler2)\n",
        "    return logger\n",
        "\n",
        "logger = get_logger('log')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1MiAz8YHUvL"
      },
      "source": [
        "import logging\n",
        "\n",
        "from lightgbm.callback import _format_eval_result\n",
        "\n",
        "\n",
        "def log_evaluation(logger, period=1, show_stdv=True, level=logging.DEBUG):\n",
        "    def _callback(env):\n",
        "        if period > 0 and env.evaluation_result_list and (env.iteration + 1) % period == 0:\n",
        "            result = '\\t'.join([_format_eval_result(x, show_stdv) for x in env.evaluation_result_list])\n",
        "            logger.info('[{}]\\t{}'.format(env.iteration+1, result))\n",
        "    _callback.order = 10\n",
        "    return _callback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91KSFS2-0J9Q"
      },
      "source": [
        "def convDictKeyToNdarray(dict):\n",
        "    return np.array([list(dict.keys())])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXqTN98ViSX5"
      },
      "source": [
        "def plot_results(name, target, pred, figsize=(6,6)):\n",
        "    plt.figure(0).clf()\n",
        "\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    fpr, tpr, thresh = metrics.roc_curve(target, pred)\n",
        "    auc = metrics.roc_auc_score(target, pred)\n",
        "    plt.title(f'{name}: {auc:0.5f}', fontsize=18)\n",
        "    plt.plot(fpr,tpr,label=name)\n",
        "    plt.legend(loc=0)\n",
        "    plt.show()\n",
        "    fig.savefig(\"result.png\")\n",
        "    #return fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfhRAnlGP0IY"
      },
      "source": [
        "def plot_FEATURES_SEARCH(results, figsize=(20, 8)):\n",
        "    plt.figure(0).clf()\n",
        "    results = np.array(results)\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "\n",
        "    x_position = np.arange(len(results[:,0]))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.bar(x_position, results[:,1].astype(np.float32), tick_label=results[:,0])\n",
        "    ylim = results[:,1].astype(np.float32)\n",
        "    if ylim.min() == ylim.max():\n",
        "        bottom=ylim.min()*0.99\n",
        "        top=ylim.max()*1.01\n",
        "    else:\n",
        "        bottom=ylim.min()-(ylim.max()-ylim.min())*0.1\n",
        "        top=ylim.max()+(ylim.max()-ylim.min())*0.1\n",
        "    ax.set_ylim(bottom=bottom, top=top)\n",
        "    plt.show()\n",
        "    fig.savefig(\"FEATURES_SEARCH.png\")\n",
        "    #return fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmrfKRgtMmuR"
      },
      "source": [
        "def seed_everything(seed_value):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "    \n",
        "    if torch.cuda.is_available(): \n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        \n",
        "seed_everything(21)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtvG47eNRGwM"
      },
      "source": [
        "**---------- Data Processing ----------**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab3Lxau5x-_F"
      },
      "source": [
        "cont_features = [\n",
        "    \"cont0\", \"cont1\", \"cont2\", \"cont3\", \"cont4\", \"cont5\", \"cont6\", \"cont7\",\n",
        "    \"cont8\", \"cont9\", \"cont10\",\n",
        "]\n",
        "cat_features = [\n",
        "    \"cat0\", \"cat1\", \"cat2\", \"cat3\", \"cat4\", \"cat5\", \"cat6\", \"cat7\",\n",
        "    \"cat8\", \"cat9\", \"cat10\", \"cat11\", \"cat12\", \"cat13\", \"cat14\", \"cat15\",\n",
        "    \"cat16\", \"cat17\", \"cat18\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKRW5Afaa2a8"
      },
      "source": [
        "def replace_outliers(data):\n",
        "    for col in cont_features:\n",
        "        Q1 = data[col].quantile(0.25)\n",
        "        Q3 = data[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        median_ = data[col].median()\n",
        "#         data[col].mask(((data[col] < Q1 - 1.5*IQR) | (data[col] > Q3 + 1.5*IQR)), median_, inplace=True)\n",
        "        # data[col] = np.where(((data[col] < Q1 - 1.5*IQR) | (data[col] > Q3 + 1.5*IQR)),\n",
        "        #                     median_, data[col])\n",
        "        #data.loc[((data[col] < Q1 - 1.5*IQR) | (data[col] > Q3 + 1.5*IQR)), col] = median_\n",
        "        data.loc[((data[col] < Q1 - 1.5*IQR)), col] = Q1 - 1.5*IQR\n",
        "        data.loc[((data[col] > Q3 + 1.5*IQR)), col] = Q3 + 1.5*IQR\n",
        "        #logger.info(f'replace_outliers : Q1 - 1.5*IQR,Q3 + 1.5*IQR')\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yznjp6Su4N4K"
      },
      "source": [
        "from category_encoders import LeaveOneOutEncoder\n",
        "def loo_encode(train_df, test_df, column):\n",
        "    loo = LeaveOneOutEncoder()\n",
        "    new_feature = \"{}_loo\".format(column)\n",
        "    loo.fit(train_df[column], train_df[\"target\"])\n",
        "    train_df[new_feature] = loo.transform(train_df[column])\n",
        "    test_df[new_feature] = loo.transform(test_df[column])\n",
        "    return new_feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUw31Lsv5jnw"
      },
      "source": [
        "def one_hot_encode(train_df, test_df, column):\n",
        "    all = pd.concat([train_df[column], test_df[column]])\n",
        "    all = pd.get_dummies(all,columns=column)\n",
        "    for c in all.columns:\n",
        "        train[c] = all[c].iloc[:train_df.shape[0]]\n",
        "        test[c] = all[c].iloc[train_df.shape[0]:]\n",
        "    return list(all.columns)\n",
        "    #train = all.iloc[:train.shape[0],:].reset_index(drop=True)\n",
        "    #train = all.iloc[:train.shape[0],:]\n",
        "    #test = all.iloc[train.shape[0]:,:].reset_index(drop=True)\n",
        "    #test = all.iloc[train.shape[0]:,:]\n",
        "    #return train, test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhr1GxcgAgd7"
      },
      "source": [
        "def label_encode(train, test):\n",
        "    for column in cat_features:\n",
        "        le = preprocessing.LabelEncoder()\n",
        "        le.fit(list(train[column].astype('str')) + list(test[column].astype('str')))\n",
        "        train[column] = le.transform(list(train[column].astype(str))) \n",
        "        test[column] = le.transform(list(test[column].astype(str))) \n",
        "    return train, test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7zewY9f8EKf"
      },
      "source": [
        "def reject_outliers(df, feature, threshold=3):\n",
        "    mean, std = np.mean(df), np.std(df)\n",
        "    z_score = np.abs((df-mean) / std)\n",
        "    good = z_score < threshold\n",
        "    return good\n",
        "#good = reject_outliers(train['target'], 'target', threshold=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGnIWmx48PNA"
      },
      "source": [
        "def reject_lof_outliers(df, feature):\n",
        "    lof = LocalOutlierFactor(n_neighbors=20, contamination=0.001, p=1)\n",
        "    good = lof.fit_predict(df) > 0.5 # change this value to set the threshold for outliers    \n",
        "    return good\n",
        "#good = reject_lof_outliers(train['target'].values.reshape(train['target'].shape[0], -1), 'target')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX8em8IYhCN5"
      },
      "source": [
        "train = pd.read_csv(input_path / 'train.csv', index_col='id')\n",
        "#train = pd.read_csv(input_path / 'train4.csv', index_col='id')\n",
        "display(train.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKpVTI0nhEqG"
      },
      "source": [
        "test = pd.read_csv(input_path / 'test.csv', index_col='id')\n",
        "#test = pd.read_csv(input_path / 'test4.csv', index_col='id')\n",
        "display(test.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a30IxJGQar6V"
      },
      "source": [
        "loo_features = []\n",
        "\n",
        "for feature in cat_features:\n",
        "    loo_features.append(loo_encode(train, test, feature))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxKvmPXT6xZB"
      },
      "source": [
        "#tmp_num = train.shape[1]\n",
        "#train, test = one_hot_encode(train, test, cat_features)\n",
        "#one_hot_features = list(train.columns[tmp_num:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2t41v_I_8uF"
      },
      "source": [
        "one_hot_features = one_hot_encode(train, test, cat_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nRDx_b-PiQb"
      },
      "source": [
        "#target = pd.read_csv(input_path / 'target.csv', index_col='id')\n",
        "#display(target.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwGPBOIWBSmK"
      },
      "source": [
        "train, test = label_encode(train, test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGeDIDFHa6z_"
      },
      "source": [
        "train = replace_outliers(train)\n",
        "test = replace_outliers(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oybeWFYBaQ1"
      },
      "source": [
        "display(train.tail())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxKl-kAHBfav"
      },
      "source": [
        "display(test.tail())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7BomEcWhKlM"
      },
      "source": [
        "target = train.pop('target')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x5PtC5lYald"
      },
      "source": [
        "submission = pd.read_csv(input_path / 'sample_submission.csv', index_col='id')\n",
        "display(submission.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGImxlw58vRm"
      },
      "source": [
        "#cat_idxs = [i for i in range(19)]\n",
        "cat_idxs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrP5xoik9aPg"
      },
      "source": [
        "#cat_dims = [i for i in train[cat_features].nunique()]\n",
        "cat_dims = [2, 15, 19, 13, 20, 84, 16, 51, 61, 19, 2, 2, 2, 2, 4, 4, 4, 4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-qltLVcfcIl"
      },
      "source": [
        "print(train.shape)\n",
        "print(test.shape)\n",
        "print(target.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSNHCONk1hKP"
      },
      "source": [
        "#train_add = pd.read_csv(input_path / 'train_add.csv', index_col='id')\n",
        "#test_add = pd.read_csv(input_path / 'test_add.csv', index_col='id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGbdkMast3zn"
      },
      "source": [
        "#train = pd.concat([train, train_add], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ig_S70BuXFv"
      },
      "source": [
        "#test = pd.concat([test, test_add], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfYqdlfluIOX"
      },
      "source": [
        "#display(train.tail())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weQ2PUYruK3O"
      },
      "source": [
        "#display(test.tail())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC6Kls1dswvo"
      },
      "source": [
        "#print(train.shape)\n",
        "#print(test.shape)\n",
        "#print(target.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKC0s-f1m7y2"
      },
      "source": [
        "**---------- Model ----------**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MtG1o6KnAKg"
      },
      "source": [
        "class Model:\n",
        "    def __init__(self, model=None):\n",
        "        self.model = model\n",
        "\n",
        "    #def set_params(self, **params):\n",
        "    #    self.params = params\n",
        "    #    _ = self.model.set_params(**params)\n",
        "\n",
        "    def set_seed(self, seed):\n",
        "        self.seed = seed\n",
        "    \n",
        "    def get_params(self):\n",
        "        return self.params\n",
        "\n",
        "    def set_init_params(self, init_params):\n",
        "        self.init_params = init_params\n",
        "\n",
        "    def get_init_params(self):\n",
        "        return self.init_params\n",
        "\n",
        "    def set_model(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.model\n",
        "    \n",
        "    def fit(self, X, Y):\n",
        "        self.model.fit(X ,Y)\n",
        "\n",
        "    def fit(self, X, Y, VX, VY):\n",
        "        self.model.fit(X ,Y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC_82Z9qoFP1"
      },
      "source": [
        "class Model_DummyRegressor(Model):\n",
        "    def __init__(self):\n",
        "        self.model = DummyRegressor()\n",
        "\n",
        "    def reset_model(self,**params):\n",
        "        del self.model\n",
        "        self.model = DummyRegressor(**params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_E1mW26E4BS"
      },
      "source": [
        "#DummyRegressor\n",
        "'''\n",
        "params_DummyRegressor = {\n",
        "    'strategy': hp.choice('strategy', ['median', 'mean']), \n",
        "}\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFfc95bL5FC1"
      },
      "source": [
        "class Model_LinearRegression(Model):\n",
        "    def __init__(self):\n",
        "        self.model = LinearRegression()\n",
        "        self.set_init_params(params_LinearRegression)\n",
        "\n",
        "    def reset_model(self,**params):\n",
        "        del self.model\n",
        "        self.model = LinearRegression(**params)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDjYMNwT5kY6"
      },
      "source": [
        "#Model_LinearRegression\n",
        "params_LinearRegression = {\n",
        "    \"normalize\": True,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqY-99n_OKGV"
      },
      "source": [
        "class Model_Ridge(Model):\n",
        "    def __init__(self):\n",
        "        self.model = CalibratedClassifierCV(\n",
        "            RidgeClassifier(),\n",
        "            cv=3,\n",
        "        )\n",
        "\n",
        "    def reset_model(self, **params):\n",
        "        del self.model\n",
        "        tmp_params = params\n",
        "        tmp_cv = tmp_params['cv']\n",
        "        del tmp_params['cv']\n",
        "        if hasattr(self, 'seed'):\n",
        "            tmp_params['random_state']=self.seed\n",
        "        elif not 'random_state' in params:\n",
        "            tmp_params['random_state']=21\n",
        "        self.params = tmp_params\n",
        "        self.model = CalibratedClassifierCV(\n",
        "            RidgeClassifier(**tmp_params),\n",
        "            cv=tmp_cv,\n",
        "        )\n",
        "        #self.model = RidgeClassifier(**tmp_params)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict_proba(X)[:,-1]\n",
        "\n",
        "    def objective(self, trial):\n",
        "        oof_preds = np.zeros(train.shape[0])\n",
        "\n",
        "        params = {\n",
        "            'alpha': trial.suggest_uniform('alpha', 0.0, 10.0),\n",
        "            'normalize': trial.suggest_categorical('normalize', [True, False]),\n",
        "            'cv': trial.suggest_int('cv', 2, 10, 1),\n",
        "        }\n",
        "\n",
        "        self.reset_model(**params)\n",
        "\n",
        "        kf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=21)\n",
        "        for tr_idx, va_idx in kf.split(train, target):\n",
        "            tr_x, va_x = train.iloc[tr_idx], train.iloc[va_idx]\n",
        "            tr_y, va_y = target.iloc[tr_idx], target.iloc[va_idx]\n",
        "\n",
        "            self.fit(tr_x, tr_y, va_x, va_y)\n",
        "            oof_preds[va_idx] = self.predict(va_x)\n",
        "\n",
        "        score = roc_auc_score(target, oof_preds)\n",
        "        return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XizzO9eNX7Uo"
      },
      "source": [
        "class Model_SGD(Model):\n",
        "    def __init__(self):\n",
        "        self.model = CalibratedClassifierCV(\n",
        "            SGDClassifier(),\n",
        "            cv=3,\n",
        "        )\n",
        "\n",
        "    def reset_model(self, **params):\n",
        "        del self.model\n",
        "        tmp_params = params\n",
        "        tmp_cv = tmp_params['cv']\n",
        "        del tmp_params['cv']\n",
        "        tmp_params['loss'] = \"squared_hinge\"\n",
        "        tmp_params['max_iter'] = 100000\n",
        "        tmp_params['n_jobs'] = -1\n",
        "        if hasattr(self, 'seed'):\n",
        "            tmp_params['random_state']=self.seed\n",
        "        elif not 'random_state' in params:\n",
        "            tmp_params['random_state']=21\n",
        "        self.params = tmp_params\n",
        "        self.model = CalibratedClassifierCV(\n",
        "            SGDClassifier(**tmp_params),\n",
        "            cv=tmp_cv,\n",
        "        )\n",
        "        #self.model = RidgeClassifier(**tmp_params)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict_proba(X)[:,-1]\n",
        "\n",
        "    def objective(self, trial):\n",
        "        oof_preds = np.zeros(train.shape[0])\n",
        "\n",
        "        params = {\n",
        "            'alpha': trial.suggest_loguniform(\"alpha\", 0.00005, 0.0005), \n",
        "            'l1_ratio': trial.suggest_uniform('l1_ratio', 0.01, 0.8),\n",
        "            'cv': trial.suggest_int('cv', 2, 6, 1),\n",
        "        }\n",
        "\n",
        "        self.reset_model(**params)\n",
        "\n",
        "        kf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=21)\n",
        "        for tr_idx, va_idx in kf.split(train, target):\n",
        "            tr_x, va_x = train.iloc[tr_idx], train.iloc[va_idx]\n",
        "            tr_y, va_y = target.iloc[tr_idx], target.iloc[va_idx]\n",
        "\n",
        "            self.fit(tr_x, tr_y, va_x, va_y)\n",
        "            oof_preds[va_idx] = self.predict(va_x)\n",
        "\n",
        "        score = roc_auc_score(target, oof_preds)\n",
        "        return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP0bkCeN8DOm"
      },
      "source": [
        "class Model_HGB(Model):\n",
        "    def __init__(self):\n",
        "        self.model = HistGradientBoostingClassifier()\n",
        "\n",
        "    def reset_model(self, **params):\n",
        "        del self.model\n",
        "        tmp_params = params\n",
        "        tmp_params['max_iter'] = 1000\n",
        "        if hasattr(self, 'seed'):\n",
        "            tmp_params['random_state']=self.seed\n",
        "        elif not 'random_state' in params:\n",
        "            tmp_params['random_state']=21\n",
        "        self.params = tmp_params\n",
        "        self.model = HistGradientBoostingClassifier(**tmp_params)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict_proba(X)[:,-1]\n",
        "\n",
        "    def objective(self, trial):\n",
        "        oof_preds = np.zeros(train.shape[0])\n",
        "\n",
        "        params = {\n",
        "            'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.3),\n",
        "            'l2_regularization': trial.suggest_uniform('l2_regularization', 0.0, 5.0),\n",
        "            'max_bins': trial.suggest_int('max_bins', 50, 255, 5),\n",
        "            'max_depth': trial.suggest_int('max_depth', 5, 50, 1),\n",
        "            'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 5, 255, 5),\n",
        "        }\n",
        "\n",
        "        self.reset_model(**params)\n",
        "\n",
        "        kf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=21)\n",
        "        for tr_idx, va_idx in kf.split(train, target):\n",
        "            tr_x, va_x = train.iloc[tr_idx], train.iloc[va_idx]\n",
        "            tr_y, va_y = target.iloc[tr_idx], target.iloc[va_idx]\n",
        "\n",
        "            self.fit(tr_x, tr_y, va_x, va_y)\n",
        "            oof_preds[va_idx] = self.predict(va_x)\n",
        "\n",
        "        score = roc_auc_score(target, oof_preds)\n",
        "        return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A40JfduxytIg"
      },
      "source": [
        "class Model_XGB(Model):\n",
        "    def __init__(self):\n",
        "        self.model = xgb.XGBClassifier()\n",
        "\n",
        "    def reset_model(self, **params):\n",
        "        del self.model\n",
        "        tmp_params = params\n",
        "        tmp_params['tree_method']='gpu_hist' \n",
        "        tmp_params['gpu_id']=0\n",
        "        tmp_params['eval_metric']='auc'\n",
        "        #tmp_params['max_depth']=25\n",
        "        tmp_params['objective']='binary:logistic'\n",
        "        if hasattr(self, 'seed'):\n",
        "            tmp_params['seed']=self.seed\n",
        "        elif not 'seed' in params:\n",
        "            tmp_params['seed']=21\n",
        "        #tmp_params['verbosity']=1\n",
        "        #tmp_params['n_estimators']=10000\n",
        "        self.params = tmp_params\n",
        "        #self.model = xgb.XGBClassifier(**tmp_params)\n",
        "\n",
        "    def fit(self, X, Y, VX, VY):\n",
        "        trn_data = xgb.DMatrix(X, label=Y)\n",
        "        val_data= xgb.DMatrix(VX, label=VY)\n",
        "        #evals = [(val_data, 'eval')]\n",
        "        evals = [(trn_data, 'train'),(val_data, 'eval')]\n",
        "        evals_result = {}\n",
        "        #self.model.fit(X ,Y, eval_metric=\"auc\", eval_set=eval_set, early_stopping_rounds=150, callbacks=[xgb.callback.record_evaluation(evals_results)], verbose=False)\n",
        "        self.model = xgb.train(self.params, trn_data, num_boost_round=10000, evals=evals, early_stopping_rounds=150, evals_result=evals_result, verbose_eval=500) \n",
        "        #return evals_results\n",
        "\n",
        "    def predict(self, X):\n",
        "        data = xgb.DMatrix(X)\n",
        "        return self.model.predict(data, ntree_limit=self.model.best_ntree_limit)\n",
        "        #return self.model.predict_proba(X)[:,-1]\n",
        "\n",
        "    def objective(self, trial):\n",
        "        oof_preds = np.zeros(train.shape[0])\n",
        "\n",
        "        params = {\n",
        "            'max_depth': trial.suggest_int('max_depth', 5, 25, 1),\n",
        "            'gamma': trial.suggest_loguniform(\"gamma\", 0.01, 10.0), \n",
        "            'min_child_weight': trial.suggest_loguniform(\"min_child_weight\", 0.01, 100.0), \n",
        "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1.0),\n",
        "            'subsample': trial.suggest_uniform('subsample', 0.1, 1.0),\n",
        "            #'learning_rate': trial.suggest_loguniform(\"learning_rate\", 0.05, 1.0), \n",
        "            'learning_rate': trial.suggest_uniform('learning_rate', 0.05, 1.0),\n",
        "            'max_delta_step': trial.suggest_loguniform(\"max_delta_step\", 0.01, 10.0), \n",
        "            'reg_alpha': trial.suggest_loguniform(\"reg_alpha\", 0.1, 10.0), \n",
        "            'reg_lambda': trial.suggest_loguniform(\"reg_lambda\", 0.1, 10.0), \n",
        "        }\n",
        "\n",
        "        self.reset_model(**params)\n",
        "\n",
        "        kf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=21)\n",
        "        #kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=21)\n",
        "        for tr_idx, va_idx in kf.split(train, target):\n",
        "            tr_x, va_x = train.iloc[tr_idx], train.iloc[va_idx]\n",
        "            tr_y, va_y = target.iloc[tr_idx], target.iloc[va_idx]\n",
        "\n",
        "            self.fit(tr_x, tr_y, va_x, va_y)\n",
        "            #print(evals_results)\n",
        "            #score = np.argmax(np.array(evals_results['validation_0']['auc']))\n",
        "            oof_preds[va_idx] = self.predict(va_x)\n",
        "\n",
        "        score = roc_auc_score(target, oof_preds)\n",
        "        self.model.__del__()\n",
        "        return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr-ld1dVFNeq"
      },
      "source": [
        "class Model_LightGBM(Model):\n",
        "    def __init__(self):\n",
        "        self.model = lgb.LGBMRegressor()\n",
        "\n",
        "    def reset_model(self, **params):\n",
        "        del self.model\n",
        "        tmp_params = params\n",
        "        #tmp_params['max_depth']=int(tmp_params['max_depth'])\n",
        "        tmp_params['num_leaves']=int(tmp_params['num_leaves'])\n",
        "        tmp_params['min_child_samples']=int(tmp_params['min_child_samples'])\n",
        "        tmp_params['min_data_per_group']=int(tmp_params['min_data_per_group'])\n",
        "        tmp_params['n_estimators']=100000\n",
        "        if hasattr(self, 'seed'):\n",
        "            tmp_params['random_state']=self.seed\n",
        "        elif not 'random_state' in params:\n",
        "            tmp_params['random_state']=21\n",
        "        self.params = tmp_params\n",
        "        self.model = lgb.LGBMRegressor(**tmp_params)\n",
        "\n",
        "    def fit(self, X, Y, VX, VY):        \n",
        "        eval_set = [(VX, VY)]\n",
        "        self.model.fit(X ,Y, eval_metric=\"rmse\", eval_set=eval_set, verbose=False, early_stopping_rounds=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN-O9aVqF1AI"
      },
      "source": [
        "'''\n",
        "LightBGM_params_space = {\n",
        "    #'learning_rate':hp.loguniform('learning_rate',np.log(0.001),np.log(0.1)),\n",
        "    'learning_rate':hp.loguniform('learning_rate',np.log(0.01),np.log(0.1)),\n",
        "    'min_child_weight':hp.loguniform('min_child_weight',np.log(0.0001),np.log(1.0)),\n",
        "    #'max_depth':hp.quniform('max_depth',3,15,1),\n",
        "    #'num_leaves':hp.quniform('num_leaves',20,50,1),\n",
        "    'num_leaves':hp.quniform('num_leaves',50,255,5),\n",
        "    'min_child_samples':hp.quniform('min_child_samples',10,30,1),\n",
        "    'reg_alpha':hp.uniform('reg_alpha',0.0,5.0),\n",
        "    'reg_lambda':hp.uniform('reg_lambda',0.0,5.0),\n",
        "    #'boosting_type': hp.choice('boosting_type', ['gbdt', 'dart', 'goss', 'rf']), \n",
        "    #'cat_smooth': hp.choice('cat_smooth', [200, 400, 600, 800, 1000]), \n",
        "    'min_data_per_group':hp.quniform('min_data_per_group',200,1000,200),\n",
        "    #'min_data_per_group': hp.choice('min_data_per_group', [200, 400, 600, 800, 1000]), \n",
        "}\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoTQ64ezdRUN"
      },
      "source": [
        "LightBGM_params = {\n",
        "    \"learning_rate\": 0.023138520618280357,\n",
        "    \"min_child_samples\": 21,\n",
        "    \"min_child_weight\": 0.007646593938515828,\n",
        "    \"num_leaves\": 210,\n",
        "    \"reg_alpha\": 4.841326955256633,\n",
        "    \"reg_lambda\": 2.3605570451667712,\n",
        "    \"n_estimators\": 10000\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hrow-AqB0V1"
      },
      "source": [
        "class Model_LightGBMwithCats(Model):\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.set_init_params(LightBGMwithCats_params)\n",
        "\n",
        "    def reset_model(self, **params):\n",
        "        tmp_params = params\n",
        "        tmp_params['n_estimators']=10000\n",
        "        if hasattr(self, 'seed'):\n",
        "            tmp_params['random_state']=self.seed\n",
        "        elif not 'random_state' in params:\n",
        "            tmp_params['random_state']=21\n",
        "        #tmp_params['objective'] = 'binary' \n",
        "        tmp_params['objective'] = 'cross_entropy' \n",
        "        tmp_params['boosting'] = 'gbdt'\n",
        "        tmp_params['metric']= 'auc'\n",
        "        self.params = tmp_params\n",
        "\n",
        "    def fit(self, X, Y, VX, VY):\n",
        "        #cats = [c for c in X.columns if X[c].dtypes=='object']\n",
        "        #trn_data = lgb.Dataset(X, label=Y, categorical_feature=cats) #-------> Specify Categorical feature for lgb\n",
        "        #val_data= lgb.Dataset(VX, label=VY, categorical_feature=cats)  #-------> Specify Categorical feature for lgb\n",
        "        trn_data = lgb.Dataset(X, label=Y, categorical_feature=cat_features) #-------> Specify Categorical feature for lgb\n",
        "        val_data= lgb.Dataset(VX, label=VY, categorical_feature=cat_features)  #-------> Specify Categorical feature for lgb\n",
        "        #logger.info(f'')\n",
        "        #callbacks = [log_evaluation(logger, period=500)]\n",
        "        evals_results = {}\n",
        "        self.model = lgb.train(self.params, trn_data, num_boost_round=10000, valid_sets=(trn_data, val_data), verbose_eval=500, early_stopping_rounds=150, callbacks=[lgb.record_evaluation(evals_results)])\n",
        "        return evals_results\n",
        "\n",
        "    def objective(self, trial):\n",
        "        oof_preds = np.zeros(train.shape[0])\n",
        "\n",
        "        params = {\n",
        "            #'learning_rate': trial.suggest_loguniform(\"learning_rate\", 0.01, 0.1), \n",
        "            'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.1),\n",
        "            'min_child_weight': trial.suggest_loguniform(\"min_child_weight\", 0.0001, 1.0), \n",
        "            'max_depth': trial.suggest_int('max_depth', 15, 50, 5),\n",
        "            'num_leaves': trial.suggest_int('num_leaves', 10, 255, 5),\n",
        "            'min_child_samples': trial.suggest_int('min_child_samples', 10, 400, 10),\n",
        "            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.0, 1.0),\n",
        "            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.0, 1.0),\n",
        "            'reg_alpha': trial.suggest_uniform('reg_alpha', 0.0, 10.0),\n",
        "            'reg_lambda': trial.suggest_uniform('reg_lambda', 0.0, 5.0),\n",
        "            'cat_smooth': trial.suggest_int('cat_smooth', 20, 100, 20),\n",
        "            'min_data_per_group': trial.suggest_int('min_data_per_group', 50, 400, 50),\n",
        "            'max_bin': trial.suggest_int('max_bin', 256, 1024, 128),\n",
        "            'bagging_freq': trial.suggest_int('bagging_freq', 0, 2, 1),\n",
        "            'cat_l2': trial.suggest_uniform('cat_l2', 5.0, 20.0),\n",
        "        }\n",
        "\n",
        "        self.reset_model(**params)\n",
        "\n",
        "        kf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=21)\n",
        "        #kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=21)\n",
        "        for tr_idx, va_idx in kf.split(train, target):\n",
        "            tr_x, va_x = train.iloc[tr_idx], train.iloc[va_idx]\n",
        "            tr_y, va_y = target.iloc[tr_idx], target.iloc[va_idx]\n",
        "\n",
        "            evals_results = self.fit(tr_x, tr_y, va_x, va_y)\n",
        "            #print(evals_results)\n",
        "            #score = np.argmax(np.array(evals_results['valid_1']['auc']))\n",
        "            oof_preds[va_idx] = self.predict(va_x)\n",
        "\n",
        "        score = roc_auc_score(target, oof_preds)\n",
        "        return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsl6D4L7sdi9"
      },
      "source": [
        "LightBGMwithCats_params = {\n",
        "    \"learning_rate\": 0.010051841635755903,\n",
        "    \"min_child_weight\": 0.8478873301463337,\n",
        "    \"max_depth\": 20,\n",
        "    \"num_leaves\": 180,\n",
        "    \"min_child_samples\": 140,\n",
        "    \"bagging_fraction\": 0.38894237049923747,\n",
        "    \"feature_fraction\": 0.25427393680284716,\n",
        "    \"reg_alpha\": 7.0525046457500205,\n",
        "    \"reg_lambda\": 0.3816397618302956,\n",
        "    \"cat_smooth\": 60,\n",
        "    \"min_data_per_group\": 250,\n",
        "    \"max_bin\": 256,\n",
        "    \"bagging_freq\": 0,\n",
        "    \"cat_l2\": 6.345830496649996\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3JGHS_MUaPh"
      },
      "source": [
        "class Model_CatBoost(Model):\n",
        "    def __init__(self):\n",
        "        self.model = cb.CatBoostClassifier()\n",
        "\n",
        "    def reset_model(self, **params):\n",
        "        del self.model\n",
        "        tmp_params = params\n",
        "        tmp_params['allow_writing_files'] = False\n",
        "        tmp_params['od_type'] = 'Iter'\n",
        "        tmp_params['grow_policy'] = 'Depthwise'\n",
        "        tmp_params['silent'] = False\n",
        "        tmp_params['eval_metric'] = 'AUC'\n",
        "        tmp_params['loss_function']=\"Logloss\"\n",
        "        tmp_params['iterations'] = 10000\n",
        "        tmp_params['task_type']=\"GPU\"\n",
        "        tmp_params['devices']=\"0\"\n",
        "        if hasattr(self, 'seed'):\n",
        "            tmp_params['random_state']=self.seed\n",
        "        elif not 'random_state' in params:\n",
        "            tmp_params['random_state']=21\n",
        "        self.params = tmp_params\n",
        "        self.model = cb.CatBoostClassifier(**tmp_params)\n",
        "\n",
        "    def fit(self, X, Y, VX, VY):\n",
        "        eval_set = [(VX, VY)]\n",
        "        #categorical_features_indices = np.where(X.dtypes == 'object')[0]\n",
        "        self.model.fit(\n",
        "            X, Y,\n",
        "            #eval_metric=\"rmse\",\n",
        "            #cat_features=categorical_features_indices,\n",
        "            cat_features=cat_features,\n",
        "            eval_set=eval_set,\n",
        "            verbose=500,\n",
        "            early_stopping_rounds=150\n",
        "        )\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict_proba(X)[:,-1]\n",
        "\n",
        "    def objective(self, trial):\n",
        "        oof_preds = np.zeros(train.shape[0])\n",
        "\n",
        "        params = {\n",
        "            #'learning_rate': trial.suggest_loguniform(\"learning_rate\", 0.01, 0.1), \n",
        "            'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.1),\n",
        "            'bagging_temperature': trial.suggest_uniform('bagging_temperature', 0.0, 2.0),\n",
        "            'depth': trial.suggest_int('depth', 6, 10, 1),\n",
        "            'od_wait': trial.suggest_int('od_wait', 10, 200, 10),\n",
        "            'l2_leaf_reg': trial.suggest_uniform('l2_leaf_reg', 1.0, 10.0),\n",
        "            'penalties_coefficient': trial.suggest_uniform('penalties_coefficient', 1.0, 3.0),\n",
        "        }\n",
        "\n",
        "        self.reset_model(**params)\n",
        "\n",
        "        kf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=21)\n",
        "        #kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=21)\n",
        "        for tr_idx, va_idx in kf.split(train, target):\n",
        "            tr_x, va_x = train.iloc[tr_idx], train.iloc[va_idx]\n",
        "            tr_y, va_y = target.iloc[tr_idx], target.iloc[va_idx]\n",
        "\n",
        "            evals_results = self.fit(tr_x, tr_y, va_x, va_y)\n",
        "            #print(evals_results)\n",
        "            #score = np.argmax(np.array(evals_results['valid_1']['auc']))\n",
        "            oof_preds[va_idx] = self.predict(va_x)\n",
        "\n",
        "        score = roc_auc_score(target, oof_preds)\n",
        "        return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T46RbkeNpLF"
      },
      "source": [
        "class Model_TabNet(Model):\n",
        "    def __init__(self):\n",
        "        self.model = TabNetClassifier()\n",
        "        self.set_init_params(TabNet_params)\n",
        "\n",
        "    def reset_model(self, **params):\n",
        "        del self.model\n",
        "\n",
        "        tmp_params = params\n",
        "        tmp_params['n_a'] = tmp_params['n_d']\n",
        "        tmp_params['optimizer_fn'] = torch.optim.Adam\n",
        "        tmp_params['optimizer_params'] = dict(lr=2e-2, weight_decay=1e-5)\n",
        "        tmp_params['mask_type'] = 'entmax'\n",
        "        tmp_params['scheduler_params'] = dict(mode=\"min\", patience=5, min_lr=1e-5, factor=0.9)\n",
        "        tmp_params['scheduler_fn'] = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
        "        tmp_params['verbose'] = 10\n",
        "        tmp_params['cat_idxs'] = cat_idxs\n",
        "        tmp_params['cat_dims'] = cat_dims\n",
        "        if hasattr(self, 'seed'):\n",
        "            tmp_params['seed']=self.seed\n",
        "        elif not 'seed' in params:\n",
        "            tmp_params['seed']=21\n",
        "        self.params = tmp_params\n",
        "        self.model = TabNetClassifier(**tmp_params)\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = X.values\n",
        "        return self.model.predict(X)\n",
        "        #return self.model.predict(X)[:,-1]\n",
        "\n",
        "    def fit(self, X, Y, VX, VY):\n",
        "        X = X.values\n",
        "        #Y = Y.values.reshape(-1, 1)\n",
        "        Y = Y.values\n",
        "        VX = VX.values\n",
        "        #VY = VY.values.reshape(-1, 1)\n",
        "        VY = VY.values\n",
        "\n",
        "        #eval_set = [(VX, VY)]\n",
        "\n",
        "        self.model.fit(\n",
        "            X_train=X,\n",
        "            y_train=Y,\n",
        "            #eval_set=eval_set,\n",
        "            eval_set=[(X, Y), (VX, VY)],\n",
        "            #eval_name = [\"val\"],\n",
        "            eval_name=['train', 'valid'],\n",
        "            eval_metric = [\"auc\"],\n",
        "            max_epochs=400,\n",
        "            patience=20,\n",
        "            #max_epochs=4,\n",
        "            #patience=2,\n",
        "            #patience=50,\n",
        "            batch_size=1024, virtual_batch_size=128,\n",
        "            num_workers=16,\n",
        "            drop_last=False\n",
        "            #loss_fn=nn.MSELoss()\n",
        "        )\n",
        "\n",
        "    def objective(self, trial):\n",
        "        oof_preds = np.zeros(train.shape[0])\n",
        "\n",
        "        params = {\n",
        "            'n_d': trial.suggest_int('n_d', 16, 32, 2),\n",
        "            'n_steps': trial.suggest_int('n_steps', 1, 3, 1),\n",
        "            'gamma': trial.suggest_uniform('gamma', 1.0, 2.0),\n",
        "            'lambda_sparse': trial.suggest_loguniform(\"lambda_sparse\", 0.0001, 1.0), \n",
        "            #'smoothing': trial.suggest_loguniform(\"smoothing\", 0.0001, 0.1), \n",
        "        }\n",
        "\n",
        "        self.reset_model(**params)\n",
        "\n",
        "        kf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=21)\n",
        "        #kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=21)\n",
        "        for tr_idx, va_idx in kf.split(train, target):\n",
        "            tr_x, va_x = train.iloc[tr_idx], train.iloc[va_idx]\n",
        "            tr_y, va_y = target.iloc[tr_idx], target.iloc[va_idx]\n",
        "\n",
        "            evals_results = self.fit(tr_x, tr_y, va_x, va_y)\n",
        "            #print(evals_results)\n",
        "            #score = np.argmax(np.array(evals_results['valid_1']['auc']))\n",
        "            oof_preds[va_idx] = self.predict(va_x)\n",
        "\n",
        "        score = roc_auc_score(target, oof_preds)\n",
        "        return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVi6s8eCqhnB"
      },
      "source": [
        "TabNet_params = {\n",
        "    \"gamma\": 1.6073252457780574,\n",
        "    \"lambda_sparse\": 3.046142378479607e-05,\n",
        "    \"n_d\": 16,\n",
        "    \"n_steps\": 2,\n",
        "    'optimizer_fn':torch.optim.Adam,\n",
        "    'optimizer_params':dict(lr=2e-2, weight_decay=1e-5),\n",
        "    'mask_type':'entmax',\n",
        "    'scheduler_params':dict(mode=\"min\", patience=5, min_lr=1e-5, factor=0.9,),\n",
        "    'scheduler_fn':torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
        "    'verbose':10,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7x4GBktx2Dc"
      },
      "source": [
        "torch.autograd.detect_anomaly = False\n",
        "torch.autograd.profiler.profile = False\n",
        "torch.autograd.profiler.emit_nvtx = False\n",
        "torch.autograd.gradcheck = False\n",
        "torch.autograd.gradgradcheck = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhKmTgy0mDB_"
      },
      "source": [
        "**---------- Learning ----------**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68WAUlUMeRWf"
      },
      "source": [
        "# Parameters\n",
        "NFOLDS = 7\n",
        "logger.info(f'NFOLDS : {NFOLDS}')\n",
        "MAX_EVALS = 100\n",
        "logger.info(f'MAX_EVALS : {MAX_EVALS}')\n",
        "RANDOM_SEEDS = 50\n",
        "logger.info(f'RANDOM_SEEDS : {MAX_EVALS}')\n",
        "###########################################\n",
        "#model = Model_DummyRegressor()\n",
        "#model = Model_Ridge()\n",
        "#model = Model_SGD()\n",
        "#model = Model_HGB()\n",
        "#model = Model_XGB()\n",
        "#model = Model_LightGBM()\n",
        "model = Model_LightGBMwithCats()\n",
        "#model = Model_CatBoost()\n",
        "#model = Model_TabNet()\n",
        "logger.info(f'MODEL : {model.__class__.__name__}')\n",
        "\n",
        "if model.__class__.__name__ == 'Model_LightGBMwithCats' or model.__class__.__name__ == 'Model_CatBoost':\n",
        "    model_features = cat_features + cont_features\n",
        "elif model.__class__.__name__ == 'Model_TabNet':\n",
        "    model_features = one_hot_features + cont_features\n",
        "else:\n",
        "    model_features = loo_features + cont_features\n",
        "train = train[model_features]\n",
        "test = test[model_features]\n",
        "###########################################\n",
        "HYPERPARAMETERS_SEARCH = False\n",
        "if HYPERPARAMETERS_SEARCH:\n",
        "    logger.info(f'HYPERPARAMETERS_SEARCH : ON')\n",
        "else:\n",
        "    logger.info(f'HYPERPARAMETERS_SEARCH : OFF')\n",
        "###########################################\n",
        "BESTPARAMETER_ESTIMATE = False\n",
        "if BESTPARAMETER_ESTIMATE:\n",
        "    logger.info(f'BESTPARAMETER_ESTIMATE : ON')\n",
        "else:\n",
        "    logger.info(f'BESTPARAMETER_ESTIMATE : OFF')\n",
        "###########################################\n",
        "FEATURES_SEARCH = False\n",
        "if FEATURES_SEARCH:\n",
        "    logger.info(f'FEATURES_SEARCH : ON')\n",
        "else:\n",
        "    logger.info(f'FEATURES_SEARCH : OFF')\n",
        "###########################################\n",
        "RANDOM_SEEDS_PREDICT = True\n",
        "if RANDOM_SEEDS_PREDICT:\n",
        "    logger.info(f'RANDOM_SEEDS_PREDICT : ON')\n",
        "else:\n",
        "    logger.info(f'RANDOM_SEEDS_PREDICT : OFF')\n",
        "###########################################\n",
        "ENSEMBLE = False\n",
        "if ENSEMBLE:\n",
        "    logger.info(f'ENSEMBLE : ON')\n",
        "else:\n",
        "    logger.info(f'ENSEMBLE : OFF')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7cFmMuBHskE"
      },
      "source": [
        "**---------- HYPERPARAMETERS_SEARCH ----------**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1WHXPCdHFpi"
      },
      "source": [
        "if HYPERPARAMETERS_SEARCH:\n",
        "    #----- Timer Set -----#\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    #----- Hyperparameters Search -----#\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(model.objective, n_trials=MAX_EVALS)\n",
        "\n",
        "    #----- Timer Stop -----#\n",
        "    execution_time = time.perf_counter() - start_time\n",
        "    print(\"Learning time:{0}\".format(execution_time/60) + \"[min]\")\n",
        "\n",
        "    #----- Log -----#\n",
        "    logger.info(f'Model : {model.__class__.__name__}')\n",
        "    logger.info(f'Learning time : {execution_time/60}[min]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJD06SEsTCxq"
      },
      "source": [
        "if HYPERPARAMETERS_SEARCH:\n",
        "    fig = optuna.visualization.plot_optimization_history(study)\n",
        "    fig.show()\n",
        "    fig.write_html(\"plot_optimization_history.html\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSimZH5G_3UU"
      },
      "source": [
        "if HYPERPARAMETERS_SEARCH:\n",
        "    fig = optuna.visualization.plot_parallel_coordinate(study)\n",
        "    fig.update_layout(width=2000, height=600)\n",
        "    fig.show()\n",
        "    fig.write_html(\"plot_parallel_coordinate.html\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHXrSmMxBZzS"
      },
      "source": [
        "if HYPERPARAMETERS_SEARCH:\n",
        "    fig = optuna.visualization.plot_slice(study)\n",
        "    fig.show()\n",
        "    fig.write_html(\"plot_slice.html\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxUrvC3bCBvS"
      },
      "source": [
        "if HYPERPARAMETERS_SEARCH:\n",
        "    fig = optuna.visualization.plot_param_importances(study)\n",
        "    fig.show()\n",
        "    fig.write_html(\"plot_param_importances.html\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF6mmCyyEES2"
      },
      "source": [
        "if HYPERPARAMETERS_SEARCH:\n",
        "    fig = optuna.visualization.plot_contour(study, params=list(optuna.importance.get_param_importances(study).keys())[0:5])\n",
        "    fig.update_layout(width=1600, height=1200)\n",
        "    fig.show()\n",
        "    fig.write_html(\"plot_contour.html\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezzo7Begzvgi"
      },
      "source": [
        "if HYPERPARAMETERS_SEARCH:\n",
        "    with open(\"best_params.json\", 'w') as f:\n",
        "        json.dump(study.best_params, f, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdUJuActQvLB"
      },
      "source": [
        "if HYPERPARAMETERS_SEARCH:\n",
        "    best_params = study.best_params\n",
        "else:\n",
        "    best_params = model.get_init_params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igPIJCFeIL_K"
      },
      "source": [
        "**---------- BESTPARAMETER_ESTIMATE ----------**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl1mQxpVH97W"
      },
      "source": [
        "if BESTPARAMETER_ESTIMATE:\n",
        "    oof_preds = np.zeros(train.shape[0])\n",
        "    test_preds = np.zeros(test.shape[0])\n",
        "\n",
        "    model.reset_model(**best_params)\n",
        "\n",
        "    kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=21)\n",
        "    for tr_idx, va_idx in kf.split(train):\n",
        "        tr_x, va_x = train.iloc[tr_idx], train.iloc[va_idx]\n",
        "        tr_y, va_y = target.iloc[tr_idx], target.iloc[va_idx]\n",
        "\n",
        "        model.fit(tr_x, tr_y, va_x, va_y)\n",
        "        #model.fit(tr_x, tr_y)\n",
        "        oof_preds[va_idx] = model.predict(va_x)\n",
        "\n",
        "        test_preds += model.predict(test) / NFOLDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw9x2cyQgR9p"
      },
      "source": [
        "if BESTPARAMETER_ESTIMATE:\n",
        "    logger.info(f'AUC : {roc_auc_score(target, oof_preds)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IILCom9NchqT"
      },
      "source": [
        "if BESTPARAMETER_ESTIMATE:\n",
        "    plot_results(model.__class__.__name__, target, oof_preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wss0aleZlbX"
      },
      "source": [
        "if BESTPARAMETER_ESTIMATE:\n",
        "    prob_true, prob_pred = calibration_curve(y_true=target, y_prob=oof_preds, n_bins=100)\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(8.0, 8.0))\n",
        "    ax1.plot(prob_pred, prob_true, marker='.', label='calibration plot', color='skyblue') # キャリプレーションプロットを作成\n",
        "    ax1.plot([0, 1], [0, 1], linestyle='--', label='ideal', color='limegreen') # 45度線をプロット\n",
        "    ax1.legend(bbox_to_anchor=(1.12, 1), loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wgvwHl0iZ83"
      },
      "source": [
        "if BESTPARAMETER_ESTIMATE:\n",
        "    submission['target'] = test_preds\n",
        "    submission.to_csv('submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQtepYAKfK1c"
      },
      "source": [
        "**---------- FEATURES_SEARCH ----------**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5li5nsXu57KF"
      },
      "source": [
        "select_features = [\n",
        "    \"cat16\", \"cat1\", \"cat10\", \"cont5\", \"cat0\", \"cat15\", \"cat8\", \"cat7\", \"cat11\", \"cont4\", \"cat4\", \"cat14\",\n",
        "    \"cont6\", \"cont2\", \"cat18\", \"cat2\", \"cat6\", \"cat17\",\n",
        "    \"cont10\", \"cont8\", \"cat9\", \"cont0\", \"cont3\", \"cont9\", \"cont7\", \"cat13\", \"cont1\", \"cat5\", \"cat12\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy6hCHORHEb_"
      },
      "source": [
        "#candidate_features = cont_features + cat_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLI2cnrXgH6t"
      },
      "source": [
        "#candidate_features = [i for i in candidate_features if i not in select_features]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmuYUkX5M-7i"
      },
      "source": [
        "candidate_features = [\n",
        "    \"cat3\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvDyHfvH6f2h"
      },
      "source": [
        "if FEATURES_SEARCH:\n",
        "    hour = 9.5\n",
        "\n",
        "    #----- Timer Set -----#\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    model = Model_LightGBMwithCats()\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i in tqdm(range(len(candidate_features))):\n",
        "        oof_preds = np.zeros(train.shape[0])\n",
        "\n",
        "        max_auc = 0\n",
        "        max_feature = ''\n",
        "\n",
        "        for f in candidate_features:\n",
        "            print(\"candidate_features : ---------------------------------------\", f)\n",
        "            select_features.append(f)\n",
        "            print(select_features)\n",
        "\n",
        "            tmp_train = train[select_features]\n",
        "\n",
        "            model.reset_model(**LightBGMwithCats_params)\n",
        "\n",
        "            #kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=21)\n",
        "            kf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=21)\n",
        "        \n",
        "            for fold_, (tr_idx, va_idx) in enumerate(kf.split(tmp_train, target)):\n",
        "                print(\"fold : ---------------------------------------\", fold_)\n",
        "                tr_x, va_x = tmp_train.iloc[tr_idx], tmp_train.iloc[va_idx]\n",
        "                tr_y, va_y = target.iloc[tr_idx], target.iloc[va_idx]\n",
        "\n",
        "                model.fit(tr_x, tr_y, va_x, va_y)\n",
        "                oof_preds[va_idx]= model.predict(va_x)\n",
        "\n",
        "            tmp_auc = roc_auc_score(target, oof_preds)\n",
        "            print('auc : ' + str(tmp_auc))\n",
        "            logger.info(f'auc : {tmp_auc}')\n",
        "            if max_auc < tmp_auc:\n",
        "                max_auc = tmp_auc\n",
        "                max_feature = f\n",
        "            select_features.remove(f)\n",
        "\n",
        "            if hour*60*60 < time.perf_counter() - start_time:\n",
        "                print('-- BREAK inner loop')\n",
        "                break\n",
        "        else:\n",
        "            results.append([max_feature, max_auc])\n",
        "            plot_FEATURES_SEARCH(results)\n",
        "            select_features.append(max_feature)\n",
        "            candidate_features.remove(max_feature)\n",
        "            continue\n",
        "        \n",
        "        print('BREAK outer loop')\n",
        "        break\n",
        "\n",
        "        #results.append([max_feature, max_auc])\n",
        "        #plot_FEATURES_SEARCH(results)\n",
        "        #select_features.append(max_feature)\n",
        "        #candidate_features.remove(max_feature)\n",
        "\n",
        "    #----- Timer Stop -----#\n",
        "    execution_time = time.perf_counter() - start_time\n",
        "    print(\"Learning time:{0}\".format(execution_time/60) + \"[min]\")\n",
        "\n",
        "    #----- Log -----#\n",
        "    logger.info(f'Learning time : {execution_time/60}[min]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXIMptdEMZ6U"
      },
      "source": [
        "if FEATURES_SEARCH:\n",
        "    plot_FEATURES_SEARCH(results)\n",
        "    pd.DataFrame(results).to_csv('FEATURES_SEARCH.csv')\n",
        "    train.to_csv('train.csv')\n",
        "    test.to_csv('test.csv')\n",
        "    target.to_csv('target.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUVaoJpO6vbk"
      },
      "source": [
        "**---------- RANDOM_SEEDS_PREDICT ----------**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMmtQHLITA-G"
      },
      "source": [
        "!mkdir TEST\n",
        "!mkdir OOF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMn62mMX64QE"
      },
      "source": [
        "if RANDOM_SEEDS_PREDICT:\n",
        "    seeds = [i for i in range(RANDOM_SEEDS)]\n",
        "    #----- Timer Set -----#\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    for i in tqdm(range(len(seeds))):\n",
        "        test_preds = np.zeros(test.shape[0])\n",
        "        oof_preds = np.zeros(train.shape[0])\n",
        "\n",
        "        seed = seeds[i]\n",
        "        model.set_seed(seed)\n",
        "\n",
        "        kf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=21)\n",
        "        for fold_, (tr_idx, va_idx) in enumerate(kf.split(train, target)):\n",
        "            print(\"fold : ---------------------------------------\", fold_)\n",
        "            tr_x, va_x = train.iloc[tr_idx], train.iloc[va_idx]\n",
        "            tr_y, va_y = target.iloc[tr_idx], target.iloc[va_idx]\n",
        "\n",
        "            model.fit(tr_x, tr_y, va_x, va_y)\n",
        "            oof_preds[va_idx]= model.predict(va_x)\n",
        "\n",
        "            test_preds += model.predict(test) / NFOLDS\n",
        "\n",
        "        tmp_auc = roc_auc_score(target, oof_preds)\n",
        "        print('auc : ' + str(tmp_auc))\n",
        "        logger.info(f'{i},{tmp_auc}')\n",
        "\n",
        "        test_preds_pass = 'TEST/' + model.__class__.__name__ + '_test_preds[' + str(i) + '].csv'\n",
        "        oof_preds_pass = 'OOF/' + model.__class__.__name__ + '_oof_preds[' + str(i) + '].csv'\n",
        "\n",
        "        np.savetxt(test_preds_pass, test_preds, delimiter=',')\n",
        "        np.savetxt(oof_preds_pass, oof_preds, delimiter=',')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY3LdWz_KPp4"
      },
      "source": [
        "**---------- ENSEMBLE ----------**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T2NSZpYKWWo"
      },
      "source": [
        "def read_pred(directory_path):\n",
        "    os.chdir(directory_path)\n",
        "    ret = {}\n",
        "    for file_name in os.listdir():\n",
        "        file_path = directory_path + '/' + file_name\n",
        "        ret[file_name] = np.loadtxt(file_path, delimiter=',')\n",
        "    return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_03-K3NRMxi"
      },
      "source": [
        "def mean_pred(pred_dict):\n",
        "    ret = np.zeros(pred_dict[list(pred_dict.keys())[0]].shape[0])\n",
        "    for key in pred_dict.keys():\n",
        "        ret += pred_dict[key] / len(pred_dict)\n",
        "    return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAN_yVtVV5UL"
      },
      "source": [
        "def stack_pred(oof_dict, test_dict):\n",
        "    tmp_train = pd.DataFrame(data=None, index=train.index, columns=None, dtype=None, copy=False)\n",
        "    tmp_test = pd.DataFrame(data=None, index=test.index, columns=None, dtype=None, copy=False)\n",
        "    for key in oof_dict.keys():\n",
        "        tmp_train[key] = oof_dict[key]\n",
        "    for key in test_dict.keys():\n",
        "        tmp_test[key] = test_dict[key]\n",
        "\n",
        "    oof_preds = np.zeros(train.shape[0])\n",
        "    test_preds = np.zeros(test.shape[0])\n",
        "\n",
        "    model = CalibratedClassifierCV(\n",
        "        RidgeClassifier(random_state=21), \n",
        "        cv=3\n",
        "    )\n",
        "\n",
        "    kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=21)\n",
        "    for tr_idx, va_idx in kf.split(tmp_train):\n",
        "        tr_x, va_x = tmp_train.iloc[tr_idx], tmp_train.iloc[va_idx]\n",
        "        tr_y, va_y = target.iloc[tr_idx], target.iloc[va_idx]\n",
        "\n",
        "        model.fit(tr_x, tr_y)\n",
        "        #model.fit(tr_x, tr_y)\n",
        "        oof_preds[va_idx] = model.predict_proba(va_x)[:,-1]\n",
        "\n",
        "        test_preds += model.predict_proba(tmp_test)[:,-1] / NFOLDS\n",
        "    \n",
        "    return oof_preds, test_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOmhozdfLK9z"
      },
      "source": [
        "if ENSEMBLE:\n",
        "    oof_dict = read_pred('/content/drive/MyDrive/Kaggle/Tabular_Playground_Series_-_Mar_2021/Data/OOF')\n",
        "    test_dict = read_pred('/content/drive/MyDrive/Kaggle/Tabular_Playground_Series_-_Mar_2021/Data/TEST')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni4DKY3IQD6r"
      },
      "source": [
        "if ENSEMBLE:\n",
        "    for key in oof_dict.keys():\n",
        "        print('{0} : {1}'.format(key,roc_auc_score(target, oof_dict[key])))\n",
        "\n",
        "    print('----------')\n",
        "    oof_mean = mean_pred(oof_dict)\n",
        "    test_mean = mean_pred(test_dict)\n",
        "    print('mean : {0}'.format(roc_auc_score(target, oof_mean)))\n",
        "\n",
        "    print('----------')\n",
        "    oof_stack, test_stack = stack_pred(oof_dict, test_dict)\n",
        "    print('stack : {0}'.format(roc_auc_score(target, oof_stack)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRYrpAdkcSYB"
      },
      "source": [
        "if ENSEMBLE:\n",
        "    submission['target'] = test_mean\n",
        "    submission.to_csv('submission_mean.csv')\n",
        "    submission['target'] = test_stack\n",
        "    submission.to_csv('submission_stack.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5ryFsnqc4jg"
      },
      "source": [
        "**---------- END ----------**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxpAe1gRypVn"
      },
      "source": [
        "if FEATURES_SEARCH:\n",
        "    LOG_PATH = '/content/drive/MyDrive/Kaggle/Tabular_Playground_Series_-_Mar_2021/Result/' + datetime.now(pytz.timezone('Asia/Tokyo')).strftime(\"%Y%m%d_%H:%M\") + '_FEATURES_SEARCH'\n",
        "elif ENSEMBLE:\n",
        "    LOG_PATH = '/content/drive/MyDrive/Kaggle/Tabular_Playground_Series_-_Mar_2021/Result/' + datetime.now(pytz.timezone('Asia/Tokyo')).strftime(\"%Y%m%d_%H:%M\") + 'ENSEMBLE'\n",
        "elif RANDOM_SEEDS_PREDICT:\n",
        "    LOG_PATH = '/content/drive/MyDrive/Kaggle/Tabular_Playground_Series_-_Mar_2021/Result/' + datetime.now(pytz.timezone('Asia/Tokyo')).strftime(\"%Y%m%d_%H:%M\") + '_' + model.__class__.__name__ + '_RANDOM_SEEDS:' + str(RANDOM_SEEDS)\n",
        "else:\n",
        "    LOG_PATH = '/content/drive/MyDrive/Kaggle/Tabular_Playground_Series_-_Mar_2021/Result/' + datetime.now(pytz.timezone('Asia/Tokyo')).strftime(\"%Y%m%d_%H:%M\") + '_' + model.__class__.__name__ + '_NFOLDS:' + str(NFOLDS)\n",
        "!mkdir $LOG_PATH\n",
        "!cp /content/log.log $LOG_PATH\n",
        "!cp /content/result.png $LOG_PATH\n",
        "!cp /content/submission.csv $LOG_PATH\n",
        "!cp /content/submission_mean.csv $LOG_PATH\n",
        "!cp /content/submission_stack.csv $LOG_PATH\n",
        "!cp /content/plot_optimization_history.html $LOG_PATH\n",
        "!cp /content/plot_parallel_coordinate.html $LOG_PATH\n",
        "!cp /content/plot_slice.html $LOG_PATH\n",
        "!cp /content/plot_param_importances.html $LOG_PATH\n",
        "!cp /content/plot_contour.html $LOG_PATH\n",
        "!cp /content/best_params.json $LOG_PATH\n",
        "!cp /content/FEATURES_SEARCH.csv $LOG_PATH\n",
        "!cp /content/FEATURES_SEARCH.png $LOG_PATH\n",
        "!cp /content/train.csv $LOG_PATH\n",
        "!cp /content/test.csv $LOG_PATH\n",
        "!cp /content/target.csv $LOG_PATH\n",
        "!cp -r /content/TEST $LOG_PATH\n",
        "!cp -r /content/OOF $LOG_PATH\n",
        "#LOG_PATH = LOG_PATH.replace('\\\\','')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}